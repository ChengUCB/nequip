# TensorFloat-32

If tensor cores are available (which are available on NVIDIA GPUs since Ampere), one can use TensorFloat-32 (TF32) to improve the speed of some matmul operations in the model in exchange for a meaningful loss of numerical precision. This is a serious trade-off that must be considered in the context of your particular workflow and needs. Refer to the [PyTorch docs](https://pytorch.org/docs/stable/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices) for more details. This page explains how users can interact with TF32 settings within the NequIP framework.

```{warning}
Be cautious when attempting to use TF32 during training and inference. While the performance gains are attractive, TF32 can be detrimental for certain atomistic modelling tasks such as structure relaxations or static point calculations.
```

## Training

During training, users can configure TF32 through [`global_options`](../configuration/config.md#global_options). An example is shown in the following config file snippet (`allow_tf32` is `false` by default).
```yaml
global_options:
  allow_tf32: false
```

```{note}
TF32 will only work for `float32` models, i.e. when `model_dtype : float32` is specified. While there's no harm in configuring TF32 to be on with `model_dtype: float64` (the TF32 state will just not affect computation), NequIP will prevent users from using such configurations as a sanity check.
```

### Dynamic TF32 Scheduling

In some cases, you may want to benefit from TF32-accelerated training while avoiding potential precision limitations during final convergence.
This effect can be achieved through a two-stage training procedure: starting with TF32 enabled for faster early training, then disabling TF32 for precise convergence.

The {class}`~nequip.train.callbacks.TF32Scheduler` callback automates this process by dynamically changing TF32 settings at specified epochs during training.

```yaml
callbacks:
  - _target_: nequip.train.callbacks.TF32Scheduler
    schedule:
      0: true      # Must match global_options.allow_tf32; enables faster early training
      100: false   # Disable TF32 at epoch 100 for precise convergence
```

## Inference

Whether TF32 is used at inference is determined by [compilation time](../getting-started/workflow.md#compilation) flags. When calling `nequip-compile`, users can specify `--tf32` or `--no-tf32`. If unspecified, the default behavior is to compile a model without TF32 (regardless of whether TF32 was used during training). This compile-time decision will determine whether TF32 is used in the various integrations such as ASE and LAMMPS when the compiled model is loaded. Consider carefully whether your downstream application is likely to be affected negatively by reduced numerical precision. We generally advise against the use of TF32 if you are running relaxations or static single-frame calculations.
