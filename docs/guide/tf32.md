# TensorFloat-32

If tensor cores are available (which are available on NVIDIA GPUs since Ampere), one can use TensorFloat-32 or TF32 to improve matmul and convolution performance at the cost of precision loss (a serious trade-off to consider in atomistic simulations). Refer to the [PyTorch docs](https://pytorch.org/docs/stable/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices) for more details. This page explains how users can interact with TF32 settings within the NequIP framework.

```{warning}
Be cautious when attempting to use TF32 during training and inference. While the performance gains are attractive, TF32 can be detrimental for certain atomistic modelling tasks such as structure relaxations that require a degree of precision TF32 may not offer.
```

## Training

During training, users can configure TF32 through `global_options` (see [config docs](./config.md#global_options)). An example is shown in the following config file snippet (`allow_tf32` is `false` by default).
```yaml
global_options:
  allow_tf32: false
```

```{note}
TF32 will only work for `float32` models, i.e. when `model_dtype : float32` is specified. While there's no harm in configuring TF32 to be on with `model_dtype: float64` (the TF32 state will just not affect computation), NequIP will prevent users from using such configurations as a sanity check.
```

```{tip}
If one seeks to benefit from TF32-accelerated training while avoiding the potential precision limitations of the model, one could consider a two-step training procedure that trains with TF32 until the model is near convergence in a first stage, and continues training to convergence without TF32 in a second stage. It may be possible to implement features that simplify such schemes, so feel free to engage with us on GitHub if you're interested.
```

## Inference

Whether TF32 is used at inference is determined by [compilation time](./workflow.md#compilation) flags. When calling `nequip-compile`, users can specify `--tf32` or `--no-tf32`. If unspecified, the default behavior is to compile a model without TF32 (regardless of whether TF32 was used during training). This compile-time decision will determine whether TF32 is used in the various integrations such as ASE and LAMMPS when the compiled model is loaded.